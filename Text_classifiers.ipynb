{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is about different text classification methods and their comparsion. Done in learning purposes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sklearn.feature_extraction.text as ftext\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './spam'\n",
    "fname = 'SMSSpamCollection.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isword(x):\n",
    "    for l in x:\n",
    "        if l.isdigit():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "with open(os.path.join(dataset_dir, fname), encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        tmp = line.split('\\t')\n",
    "        labels.append(0 if 'ham' in tmp[0] else 1)\n",
    "        dataset.append(' '.join(list(filter(isword, tmp[1].split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in a wkly comp to win FA Cup final tkts May Text FA to to receive entry question(std txt rate)T&C's apply\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n",
      "FreeMsg Hey there darling it's been week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, to rcv\n",
      "Even my brother is not like to speak with me. They treat me like aids patent.\n",
      "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press to copy your friends Callertune\n",
      "WINNER!! As a valued network customer you have been selected to receivea prize reward! To claim call Claim code Valid hours only.\n",
      "Had your mobile months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(dataset[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = ftext.CountVectorizer()\n",
    "tfidf = ftext.TfidfVectorizer()\n",
    "mul_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\tf1-measure: 0.9094202898550724\n",
      "[[1650   32]\n",
      " [  18  251]]\n",
      "tfidf\tf1-measure: 0.7688787185354691\n",
      "[[1682    0]\n",
      " [ 101  168]]\n"
     ]
    }
   ],
   "source": [
    "vectorizers = ['count', 'tfidf']\n",
    "\n",
    "for i, vectorizer in enumerate((count, tfidf)):\n",
    "    data_transformed = vectorizer.fit_transform(dataset)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_transformed, labels, test_size=0.35, random_state=0)\n",
    "    mul_nb.fit(X_train, y_train)\n",
    "    mul_nb.fit(X_train, y_train)\n",
    "    y_pred = mul_nb.predict(X_test)\n",
    "    print(f'{vectorizers[i]}\\tf1-measure: {f1_score(y_test, y_pred)}\\n{confusion_matrix(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF+PCA+SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = tfidf.fit_transform(dataset)\n",
    "pca = PCA(n_components = 50)\n",
    "data_pca = pca.fit_transform(data_transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_pca, labels, test_size=0.3, random_state=0)\n",
    "svm_clf = svm.SVC(C=100000) ### this value of C shows the biggest model quality\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-measure: 0.9130434782608695\n",
      "[[1423    8]\n",
      " [  32  210]]\n"
     ]
    }
   ],
   "source": [
    "print(f'f1-measure: {f1_score(y_test, y_pred)}\\n{confusion_matrix(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM-network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "dataset_transformed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sms in dataset:\n",
    "    words = list(filter(isword, sms.split()))\n",
    "    for idx, w in enumerate(words):\n",
    "        words[idx] = lemmatizer.lemmatize(w)\n",
    "        if any(x.isalpha() for x in words[idx]):\n",
    "            words[idx] = words[idx].replace('.', '')\n",
    "    dataset_transformed.append(nltk.word_tokenize(' '.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max([len(x) for x in dataset_transformed])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in dataset_transformed:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "    sentence += [''] * (maxlen - len(sentence))\n",
    "vocab.add('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "for idx, val in enumerate(vocab):\n",
    "    word_to_idx[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transformed = np.array(dataset_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
